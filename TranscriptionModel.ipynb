{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-09-27T07:22:35.420575Z",
     "start_time": "2023-09-27T07:22:28.143638500Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from TModel.TransformerModel import TranscriptionTransformerModel\n",
    "from Tokenizer.loaderH5 import H5GuitarTokenizer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from TUtils import random_string\n",
    "from lightning import Trainer\n",
    "import torch\n",
    "from TranscriptionDataset import TranscriptionDataset\n",
    "from TModel.Retnet.TranscriptionModel import TranscriptionRetnetModel\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c0109df4c3bf46",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-09-27T07:22:35.424490800Z",
     "start_time": "2023-09-27T07:22:35.423979100Z"
    }
   },
   "outputs": [],
   "source": [
    "datasetLocation = \"Trainsets/S_Tier_1695619803_mTokens400_mNoS5.hdf5\"\n",
    "wandbProject = \"TranscriptionModel_Test\"\n",
    "batchSize = 8\n",
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af2fe4b8411116a7",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-09-27T07:22:38.617631100Z",
     "start_time": "2023-09-27T07:22:38.255806800Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset,pipe = TranscriptionDataset.getDataPipe(\n",
    "    datasetLocation,\n",
    "    batchSize,\n",
    "    batchFirst=True\n",
    ")\n",
    "# train_pipe,test_pipe = pipe.random_split({\"train\":0.8,\"test\":0.2},42)\n",
    "\n",
    "train_dl = DataLoader(dataset=pipe,batch_size=None,num_workers=num_workers)\n",
    "# test_dl = DataLoader(dataset=test_pipe, batch_size=batchSize,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cbcd4ed084603e0",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-09-27T07:22:39.386043Z",
     "start_time": "2023-09-27T07:22:39.218553Z"
    }
   },
   "outputs": [],
   "source": [
    "model = TranscriptionRetnetModel(\n",
    "    dataset.getVocabSize(),\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    # embeddingCheckpoint=\"Models/GuitarToken/Max2Length.ckpt\"\n",
    ")\n",
    "# torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83581c2e78cdfba0",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-09-27T07:22:57.767868900Z",
     "start_time": "2023-09-27T07:22:47.576929300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mdraguve\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>.\\wandb\\run-20230927_125249-22n29l3k</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/draguve/TranscriptionModel_Test/runs/22n29l3k' target=\"_blank\">pleasant-cloud-29</a></strong> to <a href='https://wandb.ai/draguve/TranscriptionModel_Test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/draguve/TranscriptionModel_Test' target=\"_blank\">https://wandb.ai/draguve/TranscriptionModel_Test</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/draguve/TranscriptionModel_Test/runs/22n29l3k' target=\"_blank\">https://wandb.ai/draguve/TranscriptionModel_Test/runs/22n29l3k</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=wandbProject)\n",
    "wandb_logger.experiment.config.update(dataset.meta_data)\n",
    "wandb_logger.experiment.config[\"batchSize\"] = batchSize\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='train_loss',\n",
    "    dirpath=f'Models/GuitarTranscription/{random_string(10)}/',\n",
    "    filename='GuitarTranscriptionModel-{epoch:02d}-{train_loss:.2f}',\n",
    "    every_n_train_steps=1000,\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    ")\n",
    "trainer = Trainer(\n",
    "    default_root_dir='Models/',\n",
    "    max_epochs=5,\n",
    "    # profiler=\"simple\",\n",
    "    # profiler=\"pytorch\",\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    # max_time=\"00:00:02:00\"\n",
    "    # precision=\"bf16-mixed\",    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1bee6ff0571423b",
   "metadata": {
    "collapsed": false,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-09-27T07:29:11.169489100Z",
     "start_time": "2023-09-27T07:22:57.767868900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                | Params\n",
      "------------------------------------------------------\n",
      "0 | encoder       | RetnetEncoderLayers | 20.5 M\n",
      "1 | decoder       | RetnetDecoderLayers | 28.4 M\n",
      "2 | tgt_embedding | Embedding           | 3.1 M \n",
      "3 | outputLinear  | Linear              | 3.1 M \n",
      "4 | loss          | CrossEntropyLoss    | 0     \n",
      "------------------------------------------------------\n",
      "55.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "55.2 M    Total params\n",
      "220.606   Total estimated model params size (MB)\n",
      "C:\\Users\\ritwi\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84e43695f3d540f196674d22702f979f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.01 GiB (GPU 0; 8.00 GiB total capacity; 11.04 GiB already allocated; 0 bytes free; 12.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit(model\u001B[38;5;241m=\u001B[39mmodel, train_dataloaders\u001B[38;5;241m=\u001B[39mtrain_dl)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:532\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    530\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m_lightning_module \u001B[38;5;241m=\u001B[39m model\n\u001B[0;32m    531\u001B[0m _verify_strategy_supports_compile(model, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy)\n\u001B[1;32m--> 532\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_and_handle_interrupt(\n\u001B[0;32m    533\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001B[0;32m    534\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:43\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     42\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[0;32m     46\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:571\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    561\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_connector\u001B[38;5;241m.\u001B[39mattach_data(\n\u001B[0;32m    562\u001B[0m     model, train_dataloaders\u001B[38;5;241m=\u001B[39mtrain_dataloaders, val_dataloaders\u001B[38;5;241m=\u001B[39mval_dataloaders, datamodule\u001B[38;5;241m=\u001B[39mdatamodule\n\u001B[0;32m    563\u001B[0m )\n\u001B[0;32m    565\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    566\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    567\u001B[0m     ckpt_path,\n\u001B[0;32m    568\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    569\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    570\u001B[0m )\n\u001B[1;32m--> 571\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run(model, ckpt_path\u001B[38;5;241m=\u001B[39mckpt_path)\n\u001B[0;32m    573\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[0;32m    574\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:980\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    975\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[0;32m    977\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    978\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[0;32m    979\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m--> 980\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_stage()\n\u001B[0;32m    982\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    985\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1023\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1021\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_sanity_check()\n\u001B[0;32m   1022\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[1;32m-> 1023\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n\u001B[0;32m   1024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1025\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected state \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:202\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start()\n\u001B[1;32m--> 202\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madvance()\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:355\u001B[0m, in \u001B[0;36m_FitLoop.advance\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    353\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher\u001B[38;5;241m.\u001B[39msetup(combined_loader)\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 355\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepoch_loop\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:133\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.run\u001B[1;34m(self, data_fetcher)\u001B[0m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone:\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 133\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madvance(data_fetcher)\n\u001B[0;32m    134\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[0;32m    135\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:219\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.advance\u001B[1;34m(self, data_fetcher)\u001B[0m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mautomatic_optimization:\n\u001B[0;32m    218\u001B[0m         \u001B[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001B[39;00m\n\u001B[1;32m--> 219\u001B[0m         batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mautomatic_optimization\u001B[38;5;241m.\u001B[39mrun(trainer\u001B[38;5;241m.\u001B[39moptimizers[\u001B[38;5;241m0\u001B[39m], kwargs)\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    221\u001B[0m         batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_optimization\u001B[38;5;241m.\u001B[39mrun(kwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:188\u001B[0m, in \u001B[0;36m_AutomaticOptimization.run\u001B[1;34m(self, optimizer, kwargs)\u001B[0m\n\u001B[0;32m    181\u001B[0m         closure()\n\u001B[0;32m    183\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[0;32m    184\u001B[0m \u001B[38;5;66;03m# BACKWARD PASS\u001B[39;00m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;66;03m# gradient update with accumulated gradients\u001B[39;00m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 188\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step(kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_idx\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0\u001B[39m), closure)\n\u001B[0;32m    190\u001B[0m result \u001B[38;5;241m=\u001B[39m closure\u001B[38;5;241m.\u001B[39mconsume_result()\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mloss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:266\u001B[0m, in \u001B[0;36m_AutomaticOptimization._optimizer_step\u001B[1;34m(self, batch_idx, train_step_and_backward_closure)\u001B[0m\n\u001B[0;32m    263\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim_progress\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mincrement_ready()\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# model hook\u001B[39;00m\n\u001B[1;32m--> 266\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_lightning_module_hook(\n\u001B[0;32m    267\u001B[0m     trainer,\n\u001B[0;32m    268\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptimizer_step\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    269\u001B[0m     trainer\u001B[38;5;241m.\u001B[39mcurrent_epoch,\n\u001B[0;32m    270\u001B[0m     batch_idx,\n\u001B[0;32m    271\u001B[0m     optimizer,\n\u001B[0;32m    272\u001B[0m     train_step_and_backward_closure,\n\u001B[0;32m    273\u001B[0m )\n\u001B[0;32m    275\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m should_accumulate:\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim_progress\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mincrement_completed()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:146\u001B[0m, in \u001B[0;36m_call_lightning_module_hook\u001B[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[0;32m    143\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m hook_name\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpl_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 146\u001B[0m     output \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    148\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[0;32m    149\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py:1276\u001B[0m, in \u001B[0;36mLightningModule.optimizer_step\u001B[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001B[0m\n\u001B[0;32m   1238\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimizer_step\u001B[39m(\n\u001B[0;32m   1239\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1240\u001B[0m     epoch: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1243\u001B[0m     optimizer_closure: Optional[Callable[[], Any]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1244\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1245\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer`\u001B[39;00m\n\u001B[0;32m   1246\u001B[0m \u001B[38;5;124;03m    calls the optimizer.\u001B[39;00m\n\u001B[0;32m   1247\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1274\u001B[0m \u001B[38;5;124;03m                    pg[\"lr\"] = lr_scale * self.learning_rate\u001B[39;00m\n\u001B[0;32m   1275\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1276\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep(closure\u001B[38;5;241m=\u001B[39moptimizer_closure)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py:161\u001B[0m, in \u001B[0;36mLightningOptimizer.step\u001B[1;34m(self, closure, **kwargs)\u001B[0m\n\u001B[0;32m    158\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MisconfigurationException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_strategy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 161\u001B[0m step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_strategy\u001B[38;5;241m.\u001B[39moptimizer_step(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer, closure, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_on_after_step()\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m step_output\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:231\u001B[0m, in \u001B[0;36mStrategy.optimizer_step\u001B[1;34m(self, optimizer, closure, model, **kwargs)\u001B[0m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001B[39;00m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule)\n\u001B[1;32m--> 231\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39moptimizer_step(optimizer, model\u001B[38;5;241m=\u001B[39mmodel, closure\u001B[38;5;241m=\u001B[39mclosure, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision_plugin.py:116\u001B[0m, in \u001B[0;36mPrecisionPlugin.optimizer_step\u001B[1;34m(self, optimizer, model, closure, **kwargs)\u001B[0m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001B[39;00m\n\u001B[0;32m    115\u001B[0m closure \u001B[38;5;241m=\u001B[39m partial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_closure, model, optimizer, closure)\n\u001B[1;32m--> 116\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m optimizer\u001B[38;5;241m.\u001B[39mstep(closure\u001B[38;5;241m=\u001B[39mclosure, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    276\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    277\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    278\u001B[0m                                \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 280\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    281\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    283\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     32\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 33\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     35\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(prev_grad)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\optim\\adamw.py:148\u001B[0m, in \u001B[0;36mAdamW.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m closure \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    147\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39menable_grad():\n\u001B[1;32m--> 148\u001B[0m         loss \u001B[38;5;241m=\u001B[39m closure()\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m group \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_groups:\n\u001B[0;32m    151\u001B[0m     params_with_grad \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision_plugin.py:103\u001B[0m, in \u001B[0;36mPrecisionPlugin._wrap_closure\u001B[1;34m(self, model, optimizer, closure)\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_closure\u001B[39m(\n\u001B[0;32m     92\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     93\u001B[0m     model: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     94\u001B[0m     optimizer: Optimizer,\n\u001B[0;32m     95\u001B[0m     closure: Callable[[], Any],\n\u001B[0;32m     96\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m     97\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001B[39;00m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001B[39;00m\n\u001B[0;32m     99\u001B[0m \n\u001B[0;32m    100\u001B[0m \u001B[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001B[39;00m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001B[39;00m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 103\u001B[0m     closure_result \u001B[38;5;241m=\u001B[39m closure()\n\u001B[0;32m    104\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_closure(model, optimizer)\n\u001B[0;32m    105\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m closure_result\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:142\u001B[0m, in \u001B[0;36mClosure.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[Tensor]:\n\u001B[1;32m--> 142\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclosure(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\u001B[38;5;241m.\u001B[39mloss\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:128\u001B[0m, in \u001B[0;36mClosure.closure\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39menable_grad()\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclosure\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ClosureResult:\n\u001B[1;32m--> 128\u001B[0m     step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step_fn()\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m step_output\u001B[38;5;241m.\u001B[39mclosure_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    131\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarning_cache\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:315\u001B[0m, in \u001B[0;36m_AutomaticOptimization._training_step\u001B[1;34m(self, kwargs)\u001B[0m\n\u001B[0;32m    312\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\n\u001B[0;32m    314\u001B[0m \u001B[38;5;66;03m# manually capture logged metrics\u001B[39;00m\n\u001B[1;32m--> 315\u001B[0m training_step_output \u001B[38;5;241m=\u001B[39m call\u001B[38;5;241m.\u001B[39m_call_strategy_hook(trainer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[0;32m    316\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mpost_training_step()\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_result_cls\u001B[38;5;241m.\u001B[39mfrom_training_step_output(training_step_output, trainer\u001B[38;5;241m.\u001B[39maccumulate_grad_batches)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:294\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[1;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[0;32m    291\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 294\u001B[0m     output \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    296\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[0;32m    297\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:380\u001B[0m, in \u001B[0;36mStrategy.training_step\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    378\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mtrain_step_context():\n\u001B[0;32m    379\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, TrainingStep)\n\u001B[1;32m--> 380\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtraining_step(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Github\\MusicTranscription\\TModel\\Retnet\\TranscriptionModel.py:57\u001B[0m, in \u001B[0;36mTranscriptionRetnetModel.training_step\u001B[1;34m(self, batch, batch_idx)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtraining_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, batch_idx):\n\u001B[0;32m     56\u001B[0m     all_mels, src_mask, src_pad_mask, all_tokens, tgt_mask, tgt_pad_mask, tokens_out \u001B[38;5;241m=\u001B[39m batch\n\u001B[1;32m---> 57\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(all_mels, src_pad_mask, all_tokens, tgt_pad_mask)\n\u001B[0;32m     58\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss(logits\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, logits\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]), tokens_out\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m, loss, prog_bar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\Github\\MusicTranscription\\TModel\\Retnet\\TranscriptionModel.py:51\u001B[0m, in \u001B[0;36mTranscriptionRetnetModel.forward\u001B[1;34m(self, src_emb, src_padding_mask, trg, tgt_padding_mask)\u001B[0m\n\u001B[0;32m     49\u001B[0m src_emb_pos \u001B[38;5;241m=\u001B[39m (src_emb)\n\u001B[0;32m     50\u001B[0m tgt_emb_pos \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtgt_embedding(trg))\n\u001B[1;32m---> 51\u001B[0m mem \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder\u001B[38;5;241m.\u001B[39mforward(src_emb_pos)\n\u001B[0;32m     52\u001B[0m outs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder\u001B[38;5;241m.\u001B[39mforward(tgt_emb_pos, mem)\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutputLinear(outs)\n",
      "File \u001B[1;32m~\\Github\\MusicTranscription\\TModel\\Retnet\\RetNetLayer.py:80\u001B[0m, in \u001B[0;36mRetnetEncoderLayers.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m---> 80\u001B[0m         x \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mforward(x)\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\Github\\MusicTranscription\\TModel\\Retnet\\RetNetLayer.py:30\u001B[0m, in \u001B[0;36mRetNetEncoderLayer.forward\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m     27\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;124;03m    X: (batch_size, sequence_length, hidden_size)\u001B[39;00m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 30\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretention_call(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norms_1(X)) \u001B[38;5;241m+\u001B[39m X\n\u001B[0;32m     31\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mffn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norms_2(X)) \u001B[38;5;241m+\u001B[39m X\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m X\n",
      "File \u001B[1;32m~\\Github\\MusicTranscription\\TModel\\Retnet\\RetNetLayer.py:23\u001B[0m, in \u001B[0;36mRetNetEncoderLayer.retention_call\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mretention_call\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[1;32m---> 23\u001B[0m     X, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretention\u001B[38;5;241m.\u001B[39mforward_parallel(X, X, X)\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m X\n",
      "File \u001B[1;32m~\\Github\\MusicTranscription\\TModel\\Retnet\\decoder_retention.py:412\u001B[0m, in \u001B[0;36mMultiScaleRetention.forward_parallel\u001B[1;34m(self, query, key, value, need_weights)\u001B[0m\n\u001B[0;32m    409\u001B[0m k \u001B[38;5;241m=\u001B[39m rearrange(k, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(b h) n d -> b h n d\u001B[39m\u001B[38;5;124m\"\u001B[39m, h\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads)\n\u001B[0;32m    411\u001B[0m \u001B[38;5;66;03m# Apply retention then group norm.\u001B[39;00m\n\u001B[1;32m--> 412\u001B[0m retention, weights \u001B[38;5;241m=\u001B[39m retention_parallel(q, k, v, need_weights\u001B[38;5;241m=\u001B[39mneed_weights)\n\u001B[0;32m    413\u001B[0m \u001B[38;5;66;03m# To apply group norm in an equivalent way to the recurrent formulation,\u001B[39;00m\n\u001B[0;32m    414\u001B[0m \u001B[38;5;66;03m# we fold the sequence dimension into the batch dimension.  Otherwise,\u001B[39;00m\n\u001B[0;32m    415\u001B[0m \u001B[38;5;66;03m# normalization would be applied over the entire input sequence.\u001B[39;00m\n\u001B[0;32m    416\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m retention\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\Github\\MusicTranscription\\TModel\\Retnet\\decoder_retention.py:272\u001B[0m, in \u001B[0;36mretention_parallel\u001B[1;34m(query, key, value, scale, decay_gammas, need_weights)\u001B[0m\n\u001B[0;32m    269\u001B[0m key \u001B[38;5;241m=\u001B[39m key \u001B[38;5;241m/\u001B[39m scale\n\u001B[0;32m    271\u001B[0m similarity \u001B[38;5;241m=\u001B[39m einsum(query, key, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb h n d, b h s d -> b h n s\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 272\u001B[0m similarity \u001B[38;5;241m=\u001B[39m similarity \u001B[38;5;241m*\u001B[39m rearrange(decay_mask, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh n s -> () h n s\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    273\u001B[0m retention \u001B[38;5;241m=\u001B[39m einsum(similarity, value, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb h n s, b h s d -> b h n d\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    275\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m need_weights:\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 1.01 GiB (GPU 0; 8.00 GiB total capacity; 11.04 GiB already allocated; 0 bytes free; 12.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=model, train_dataloaders=train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b16e41-4f3a-4ed1-a2e8-4ec9230ca7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"Models/GuitarTranscription/5s400Tokens_1e-5/smallDataset5epoch.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6b77b042d2c9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
