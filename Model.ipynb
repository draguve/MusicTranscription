{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda : True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "from SongDataset import SongDataset\n",
    "from TranscriptionModel import GuitarModel\n",
    "from torch import nn\n",
    "import os\n",
    "import logging\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# check if tensorflow is working correctly\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "print(f\"Cuda : {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SAMPLE_RATE = 44100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from SongDataset import GuitarCollater\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    ")\n",
    "dataset = SongDataset(\"test.hdf5\", mel_spectrogram, sampleRate=SAMPLE_RATE)\n",
    "collate_fn = GuitarCollater(dataset.pad_token)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 32\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model = GuitarModel((BATCH_SIZE, 2, 128, 87),\n",
    "                        emb_size=EMB_SIZE,\n",
    "                        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "                        num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "                        multi_head_attention_size=NHEAD,\n",
    "                        dim_feedforward=FFN_HID_DIM,\n",
    "                        tgt_vocab_size=dataset.vocabSize)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=dataset.pad_token)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class CheckpointSaver:\n",
    "    def __init__(self, dirpath, decreasing=True, top_n=5):\n",
    "        \"\"\"\n",
    "        dirpath: Directory path where to store all model weights\n",
    "        decreasing: If decreasing is `True`, then lower metric is better\n",
    "        top_n: Total number of models to track based on validation metric value\n",
    "        \"\"\"\n",
    "        if not os.path.exists(dirpath): os.makedirs(dirpath)\n",
    "        self.dirpath = dirpath\n",
    "        self.top_n = top_n\n",
    "        self.decreasing = decreasing\n",
    "        self.top_model_paths = []\n",
    "        self.best_metric_val = np.Inf if decreasing else -np.Inf\n",
    "\n",
    "    def __call__(self, model, epoch, metric_val):\n",
    "        model_path = os.path.join(self.dirpath, model.__class__.__name__ + f'_epoch{epoch}.pt')\n",
    "        save = metric_val<self.best_metric_val if self.decreasing else metric_val>self.best_metric_val\n",
    "        if save:\n",
    "            logging.info(f\"Current metric value better than {metric_val} better than best {self.best_metric_val}, saving model at {model_path}\")\n",
    "            self.best_metric_val = metric_val\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            self.top_model_paths.append({'path': model_path, 'score': metric_val})\n",
    "            self.top_model_paths = sorted(self.top_model_paths, key=lambda o: o['score'], reverse=not self.decreasing)\n",
    "        if len(self.top_model_paths)>self.top_n:\n",
    "            self.cleanup()\n",
    "\n",
    "    def cleanup(self):\n",
    "        to_remove = self.top_model_paths[self.top_n:]\n",
    "        logging.info(f\"Removing extra models.. {to_remove}\")\n",
    "        for o in to_remove:\n",
    "            os.remove(o['path'])\n",
    "        self.top_model_paths = self.top_model_paths[:self.top_n]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(dataset, [0.9,0.1], generator=torch.Generator().manual_seed(42))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, epoch):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE,shuffle=True,num_workers=4,collate_fn=collate_fn)\n",
    "\n",
    "    for spectrogram, tuning, tokens in tqdm(train_dataloader,desc=f\"Epoch {epoch}\"):\n",
    "        spectrogram = spectrogram.to(DEVICE)\n",
    "        tuning = tuning.to(DEVICE)\n",
    "        tokens = tokens.to(DEVICE)\n",
    "\n",
    "        tokens_input = tokens[:, :-1]\n",
    "        tokens_expected = tokens[:, 1:]\n",
    "\n",
    "        target_mask, token_padding_mask = model.create_masks(tokens_input)\n",
    "\n",
    "        target_mask = target_mask.to(DEVICE)\n",
    "        token_padding_mask = token_padding_mask.to(DEVICE)\n",
    "\n",
    "        logits = model(spectrogram, tuning, tokens_input, target_mask, token_padding_mask)\n",
    "        logits = logits.permute(1, 2, 0)\n",
    "\n",
    "        loss = loss_fn(logits, tokens_expected)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model,epoch,checkpoint_saver):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE,shuffle=False,num_workers=4,collate_fn=collate_fn)\n",
    "    for spectrogram, tuning, tokens in tqdm(val_dataloader,desc=f\"Eval {epoch}\"):\n",
    "        spectrogram = spectrogram.to(DEVICE)\n",
    "        tuning = tuning.to(DEVICE)\n",
    "        tokens = tokens.to(DEVICE)\n",
    "\n",
    "        tokens_input = tokens[:, :-1]\n",
    "        tokens_expected = tokens[:, 1:]\n",
    "\n",
    "        target_mask, token_padding_mask = model.create_masks(tokens_input)\n",
    "        target_mask = target_mask.to(DEVICE)\n",
    "        token_padding_mask = token_padding_mask.to(DEVICE)\n",
    "        logits = model(spectrogram, tuning, tokens_input, target_mask, token_padding_mask)\n",
    "        logits = logits.permute(1, 2, 0)\n",
    "\n",
    "        loss = loss_fn(logits, tokens_expected)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item()\n",
    "\n",
    "    losses = losses/len(val_dataloader)\n",
    "    checkpoint_saver(model, epoch, losses)\n",
    "    return losses / len(val_dataloader)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch 1:   0%|          | 0/18841 [00:05<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2478ab6ae6d94796a7ebda7da39b9b24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_EPOCHS = 18\n",
    "checkpoint_saver = CheckpointSaver(dirpath='./model_weights', decreasing=True, top_n=1)\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer,epoch)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model,epoch,checkpoint_saver)\n",
    "    print(f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
