{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda : True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "from SongDataset import SongDataset\n",
    "from TranscriptionModel import GuitarModel\n",
    "from torch import nn\n",
    "import os\n",
    "import logging\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from TUtils import random_string\n",
    "from Tokenizer import GuitarTokenizer\n",
    "from SongDataset import ArrangementUtils\n",
    "\n",
    "\n",
    "\n",
    "# check if tensorflow is working correctly\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "print(f\"Cuda : {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SAMPLE_RATE = 44100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from SongDataset import GuitarCollater\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    ")\n",
    "dataset = SongDataset(\"test.hdf5\", mel_spectrogram, sampleRate=SAMPLE_RATE)\n",
    "collate_fn = GuitarCollater(dataset.pad_token)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 16\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "epoch = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "model = GuitarModel((BATCH_SIZE, 2, 128, 87),\n",
    "                        emb_size=EMB_SIZE,\n",
    "                        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "                        num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "                        multi_head_attention_size=NHEAD,\n",
    "                        dim_feedforward=FFN_HID_DIM,\n",
    "                        tgt_vocab_size=dataset.vocabSize)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=dataset.pad_token)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class CheckpointSaver:\n",
    "    def __init__(self, dirpath, decreasing=True, top_n=5):\n",
    "        \"\"\"\n",
    "        dirpath: Directory path where to store all model weights\n",
    "        decreasing: If decreasing is `True`, then lower metric is better\n",
    "        top_n: Total number of models to track based on validation metric value\n",
    "        \"\"\"\n",
    "        if not os.path.exists(dirpath): os.makedirs(dirpath)\n",
    "        self.dirpath = dirpath\n",
    "        self.top_n = top_n\n",
    "        self.decreasing = decreasing\n",
    "        self.top_model_paths = []\n",
    "        self.best_metric_val = np.Inf if decreasing else -np.Inf\n",
    "        self.run_id = random_string()\n",
    "\n",
    "    def __call__(self, model, epoch, metric_val,optimizer,force_save=False):\n",
    "        model_path = os.path.join(self.dirpath, model.__class__.__name__ + f'_epoch{epoch}_run{self.run_id}.pt')\n",
    "        save = metric_val<self.best_metric_val if self.decreasing else metric_val>self.best_metric_val\n",
    "        if save or force_save:\n",
    "            logging.info(f\"Current metric value better than {metric_val} better than best {self.best_metric_val}, saving model at {model_path}\")\n",
    "            self.best_metric_val = metric_val\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            # 'loss': loss,\n",
    "            }, model_path)\n",
    "            self.top_model_paths.append({'path': model_path, 'score': metric_val})\n",
    "            self.top_model_paths = sorted(self.top_model_paths, key=lambda o: o['score'], reverse=not self.decreasing)\n",
    "        if len(self.top_model_paths)>self.top_n:\n",
    "            self.cleanup()\n",
    "\n",
    "    def cleanup(self):\n",
    "        to_remove = self.top_model_paths[self.top_n:]\n",
    "        logging.info(f\"Removing extra models.. {to_remove}\")\n",
    "        for o in to_remove:\n",
    "            os.remove(o['path'])\n",
    "        self.top_model_paths = self.top_model_paths[:self.top_n]\n",
    "\n",
    "def load_model(PATH):\n",
    "    global model\n",
    "    global optimizer\n",
    "    global epoch\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']+1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(dataset, [0.9,0.1], generator=torch.Generator().manual_seed(47))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, epoch):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    count = 0\n",
    "    train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE,shuffle=True,num_workers=8,collate_fn=collate_fn)\n",
    "\n",
    "    for spectrogram, tuning, tokens in (pbar := tqdm(train_dataloader,desc=f\"Epoch {epoch}\")):\n",
    "        spectrogram = spectrogram.to(DEVICE)\n",
    "        tuning = tuning.to(DEVICE)\n",
    "        tokens = tokens.to(DEVICE)\n",
    "\n",
    "        tokens_input = tokens[:, :-1]\n",
    "        tokens_expected = tokens[:, 1:]\n",
    "\n",
    "        target_mask, token_padding_mask = model.create_masks(tokens_input)\n",
    "\n",
    "        target_mask = target_mask.to(DEVICE)\n",
    "        token_padding_mask = token_padding_mask.to(DEVICE)\n",
    "\n",
    "        logits = model(spectrogram, tuning, tokens_input, target_mask, token_padding_mask)\n",
    "        logits = logits.permute(1, 2, 0)\n",
    "\n",
    "        loss = loss_fn(logits, tokens_expected)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item()\n",
    "        count+=1\n",
    "        pbar.set_postfix({\"loss\":losses/count})\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model,epoch,checkpoint_saver,optimizer):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    count=0\n",
    "\n",
    "    val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE,shuffle=True,num_workers=8,collate_fn=collate_fn)\n",
    "    for spectrogram, tuning, tokens in (pbar := tqdm(val_dataloader,desc=f\"Eval {epoch}\")):\n",
    "        spectrogram = spectrogram.to(DEVICE)\n",
    "        tuning = tuning.to(DEVICE)\n",
    "        tokens = tokens.to(DEVICE)\n",
    "\n",
    "        tokens_input = tokens[:, :-1]\n",
    "        tokens_expected = tokens[:, 1:]\n",
    "\n",
    "        target_mask, token_padding_mask = model.create_masks(tokens_input)\n",
    "        target_mask = target_mask.to(DEVICE)\n",
    "        token_padding_mask = token_padding_mask.to(DEVICE)\n",
    "        logits = model(spectrogram, tuning, tokens_input, target_mask, token_padding_mask)\n",
    "        logits = logits.permute(1, 2, 0)\n",
    "\n",
    "        loss = loss_fn(logits, tokens_expected)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item()\n",
    "        count+=1\n",
    "        pbar.set_postfix({\"loss\":losses/count})\n",
    "\n",
    "    losses = losses/len(val_dataloader)\n",
    "    checkpoint_saver(model, epoch, losses,optimizer,True)\n",
    "    return losses\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "load_model(\"./model_weights/GuitarModel_epoch1.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch 1:   0%|          | 0/37681 [00:13<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8055ae834ff742d084f04b4050b24810"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 1:   0%|          | 0/4187 [00:13<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e089d1a32eee41cebf2343ae7d4c1218"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 1.941, Val loss: 1.580, Epoch time = 2891.220s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 2:   0%|          | 0/37681 [00:11<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db8edf48a1b34e3892ef54845840e459"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 2:   0%|          | 0/4187 [00:12<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "237ca2fa79ce480da3759e13fc1555b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 1.599, Val loss: 1.510, Epoch time = 2817.083s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 3:   0%|          | 0/37681 [00:11<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66483e96efdc47999dea8f0a34993687"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 3:   0%|          | 0/4187 [02:13<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28b53cd8a22e4f3b96e3a76105d152fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 1.548, Val loss: 1.463, Epoch time = 2846.529s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 4:   0%|          | 0/37681 [00:11<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b12b7b766aa4fbda4618d875f89d6be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 4:   0%|          | 0/4187 [00:12<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b346993067824e909ff568c9a1bff854"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train loss: 1.510, Val loss: 1.420, Epoch time = 2837.904s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 5:   0%|          | 0/37681 [01:32<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3cd1bd6b6d3c4c75b06f6631acd8446c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 5:   0%|          | 0/4187 [00:12<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18079b3165bd4d8999c87a8ce19bfd45"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 1.480, Val loss: 1.387, Epoch time = 2917.479s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 6:   0%|          | 0/37681 [00:11<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7544d1805874eaf968776191c93ae07"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 6:   0%|          | 0/4187 [00:12<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "591d28f7212e4a0b8e7f39e00f72cf64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train loss: 1.455, Val loss: 1.358, Epoch time = 2816.632s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 7:   0%|          | 0/37681 [00:10<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6d5528951ca4a48919a5ef724cab152"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 7:   0%|          | 0/4187 [00:11<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9dc662553bdb4f5fa3f19fd6791a4e2f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train loss: 1.436, Val loss: 1.337, Epoch time = 2811.552s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 8:   0%|          | 0/37681 [00:11<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "295998254b22433baa3f81b0db7ee444"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 8:   0%|          | 0/4187 [00:12<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6beb5adac7d42f2b4434ead299e66f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train loss: 1.420, Val loss: 1.317, Epoch time = 2832.241s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 9:   0%|          | 0/37681 [00:11<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "726b47f046a84707991a232676f84337"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 9:   0%|          | 0/4187 [00:12<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afadfddfdebc48c0b150aa4004178f3d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train loss: 1.404, Val loss: 1.297, Epoch time = 2832.540s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 10:   0%|          | 0/37681 [00:12<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49c4df6c5ed940ff80c3a3a6cb4bc5ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 86028) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mEmpty\u001B[0m                                     Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1120\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1119\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1120\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\multiprocessing\\queues.py:114\u001B[0m, in \u001B[0;36mQueue.get\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll(timeout):\n\u001B[1;32m--> 114\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll():\n",
      "\u001B[1;31mEmpty\u001B[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epoch, NUM_EPOCHS\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m      5\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[1;32m----> 6\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     tb\u001B[38;5;241m.\u001B[39madd_scalar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain Loss\u001B[39m\u001B[38;5;124m\"\u001B[39m, train_loss, epoch)\n\u001B[0;32m      8\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m timer()\n",
      "Cell \u001B[1;32mIn[8], line 7\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, optimizer, epoch)\u001B[0m\n\u001B[0;32m      4\u001B[0m count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m      5\u001B[0m train_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(train_set, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE,shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m,collate_fn\u001B[38;5;241m=\u001B[39mcollate_fn)\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m spectrogram, tuning, tokens \u001B[38;5;129;01min\u001B[39;00m (pbar \u001B[38;5;241m:=\u001B[39m tqdm(train_dataloader,desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)):\n\u001B[0;32m      8\u001B[0m     spectrogram \u001B[38;5;241m=\u001B[39m spectrogram\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[0;32m      9\u001B[0m     tuning \u001B[38;5;241m=\u001B[39m tuning\u001B[38;5;241m.\u001B[39mto(DEVICE)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\tqdm\\notebook.py:259\u001B[0m, in \u001B[0;36mtqdm_notebook.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    258\u001B[0m     it \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m(tqdm_notebook, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__iter__\u001B[39m()\n\u001B[1;32m--> 259\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m it:\n\u001B[0;32m    260\u001B[0m         \u001B[38;5;66;03m# return super(tqdm...) will not catch exception\u001B[39;00m\n\u001B[0;32m    261\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m    262\u001B[0m \u001B[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\tqdm\\std.py:1195\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1192\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1194\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1195\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1196\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1197\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1316\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1313\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[0;32m   1315\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1316\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1317\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[0;32m   1319\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1282\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1278\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[0;32m   1279\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[0;32m   1280\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1281\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m-> 1282\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1283\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[0;32m   1284\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(failed_workers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1132\u001B[0m     pids_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mstr\u001B[39m(w\u001B[38;5;241m.\u001B[39mpid) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m failed_workers)\n\u001B[1;32m-> 1133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataLoader worker (pid(s) \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) exited unexpectedly\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(pids_str)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m   1134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, queue\u001B[38;5;241m.\u001B[39mEmpty):\n\u001B[0;32m   1135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: DataLoader worker (pid(s) 86028) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 18\n",
    "tb = SummaryWriter()\n",
    "checkpoint_saver = CheckpointSaver(dirpath='./model_weights2', decreasing=True, top_n=5)\n",
    "for epoch in range(epoch, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer,epoch)\n",
    "    tb.add_scalar(\"Train Loss\", train_loss, epoch)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model,epoch,checkpoint_saver,optimizer)\n",
    "    tb.add_scalar(\"Val Loss\", train_loss, epoch)\n",
    "    print(f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\")\n",
    "    for name, weight in model.named_parameters():\n",
    "        tb.add_histogram(name,weight, epoch)\n",
    "        tb.add_histogram(f'{name}.grad',weight.grad, epoch)\n",
    "tb.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def predict(model,spectrogram : torch.tensor,tuning : torch.tensor,arrangement : str,capo : float):\n",
    "    model.eval()\n",
    "    Tokenizer = GuitarTokenizer(dataset.spectrogramSizeInSeconds,dataset.numberOfTimeTokensPerSecond)\n",
    "    SOS_token = Tokenizer.sosToken\n",
    "    EOS_token = Tokenizer.eosToken\n",
    "    max_length = dataset.maxTokens\n",
    "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=DEVICE)\n",
    "    tuningAndArrangement = ArrangementUtils.getArrangementTensor(tuning,arrangement,capo)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Get source mask\n",
    "        target_mask, token_padding_mask = model.create_masks(y_input)\n",
    "        target_mask = target_mask.to(DEVICE)\n",
    "        token_padding_mask = token_padding_mask.to(DEVICE)\n",
    "\n",
    "        pred = model(spectrogram, tuningAndArrangement, y_input, target_mask, token_padding_mask)\n",
    "\n",
    "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
    "        next_item = torch.tensor([[next_item]], device=DEVICE)\n",
    "\n",
    "        # Concatenate previous input with predicted best word\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.view(-1).tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
