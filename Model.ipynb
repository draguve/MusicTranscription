{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda : True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "from SongDataset import SongDataset\n",
    "from TranscriptionModel import GuitarModel\n",
    "from torch import nn\n",
    "import os\n",
    "import logging\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from TUtils import random_string\n",
    "from Tokenizer import GuitarTokenizer\n",
    "from SongDataset import ArrangementUtils\n",
    "import wandb\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "\n",
    "\n",
    "# check if tensorflow is working correctly\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "print(f\"Cuda : {torch.cuda.is_available()}\")\n",
    "\n",
    "# os.environ['WANDB_MODE'] = 'offline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#TODO: load this from dataset file\n",
    "SAMPLE_RATE = 44100\n",
    "DatasetFile = \"./Trainsets/no_silence.hdf5\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from SongDataset import GuitarCollater\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    ")\n",
    "dataset = SongDataset(DatasetFile, sampleRate=SAMPLE_RATE)\n",
    "collate_fn = GuitarCollater(dataset.pad_token)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 16\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "LEARNING_RATE = 0.0001\n",
    "epoch = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "model = GuitarModel((BATCH_SIZE, 2, 128, 87),\n",
    "                    mel_spectrogram,\n",
    "                    emb_size=EMB_SIZE,\n",
    "                    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "                    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "                    multi_head_attention_size=NHEAD,\n",
    "                    dim_feedforward=FFN_HID_DIM,\n",
    "                    tgt_vocab_size=dataset.vocabSize)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=dataset.pad_token)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n",
    "metric = MulticlassF1Score(num_classes=int(dataset.vocabSize)).to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class CheckpointSaver:\n",
    "    def __init__(self, dirpath, decreasing=True, top_n=5):\n",
    "        \"\"\"\n",
    "        dirpath: Directory path where to store all model weights\n",
    "        decreasing: If decreasing is `True`, then lower metric is better\n",
    "        top_n: Total number of models to track based on validation metric value\n",
    "        \"\"\"\n",
    "        if not os.path.exists(dirpath): os.makedirs(dirpath)\n",
    "        self.dirpath = dirpath\n",
    "        self.top_n = top_n\n",
    "        self.decreasing = decreasing\n",
    "        self.top_model_paths = []\n",
    "        self.best_metric_val = np.Inf if decreasing else -np.Inf\n",
    "        self.run_id = random_string()\n",
    "\n",
    "    def __call__(self, model, epoch, metric_val,optimizer,force_save=False):\n",
    "        model_path = os.path.join(self.dirpath, model.__class__.__name__ + f'_epoch{epoch}_run{self.run_id}.pt')\n",
    "        save = metric_val<self.best_metric_val if self.decreasing else metric_val>self.best_metric_val\n",
    "        if save or force_save:\n",
    "            logging.info(f\"Current metric value better than {metric_val} better than best {self.best_metric_val}, saving model at {model_path}\")\n",
    "            self.best_metric_val = metric_val\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            # 'loss': loss,\n",
    "            }, model_path)\n",
    "            self.top_model_paths.append({'path': model_path, 'score': metric_val})\n",
    "            self.top_model_paths = sorted(self.top_model_paths, key=lambda o: o['score'], reverse=not self.decreasing)\n",
    "        if len(self.top_model_paths)>self.top_n:\n",
    "            self.cleanup()\n",
    "\n",
    "    def cleanup(self):\n",
    "        to_remove = self.top_model_paths[self.top_n:]\n",
    "        logging.info(f\"Removing extra models.. {to_remove}\")\n",
    "        for o in to_remove:\n",
    "            os.remove(o['path'])\n",
    "        self.top_model_paths = self.top_model_paths[:self.top_n]\n",
    "\n",
    "def load_model(PATH):\n",
    "    global model\n",
    "    global optimizer\n",
    "    global epoch\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']+1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(dataset, [0.9,0.1], generator=torch.Generator().manual_seed(47))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, epoch):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    count = 0\n",
    "    f1_accuracy = 0\n",
    "    train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE,shuffle=True,num_workers=6,collate_fn=collate_fn)\n",
    "\n",
    "    for spectrogram, tuning, tokens in (pbar := tqdm(train_dataloader,desc=f\"Epoch {epoch}\")):\n",
    "        spectrogram = spectrogram.to(DEVICE)\n",
    "        tuning = tuning.to(DEVICE)\n",
    "        tokens = tokens.to(DEVICE)\n",
    "\n",
    "        tokens_input = tokens[:, :-1]\n",
    "        tokens_expected = tokens[:, 1:]\n",
    "\n",
    "        target_mask, token_padding_mask = model.create_masks(tokens_input)\n",
    "\n",
    "        target_mask = target_mask.to(DEVICE)\n",
    "        token_padding_mask = token_padding_mask.to(DEVICE)\n",
    "\n",
    "        logits = model(spectrogram, tuning, tokens_input, target_mask, token_padding_mask)\n",
    "        logits = logits.permute(1, 2, 0)\n",
    "\n",
    "        loss = loss_fn(logits, tokens_expected)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item()\n",
    "        f1_accuracy += metric(logits,tokens_expected)\n",
    "        count+=1\n",
    "        metrics_to_save = {\n",
    "            \"train_loss\": losses/count,\n",
    "            \"f1_acc_train\":f1_accuracy/count\n",
    "        }\n",
    "        if count % config[\"LogInterval\"] == 0:\n",
    "            wandb.log(metrics_to_save)\n",
    "        pbar.set_postfix(metrics_to_save)\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model,epoch,checkpoint_saver,optimizer):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    count=0\n",
    "    f1_accuracy = 0\n",
    "\n",
    "    val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE,shuffle=True,num_workers=6,collate_fn=collate_fn)\n",
    "    for spectrogram, tuning, tokens in (pbar := tqdm(val_dataloader,desc=f\"Eval {epoch}\")):\n",
    "        spectrogram = spectrogram.to(DEVICE)\n",
    "        tuning = tuning.to(DEVICE)\n",
    "        tokens = tokens.to(DEVICE)\n",
    "\n",
    "        tokens_input = tokens[:, :-1]\n",
    "        tokens_expected = tokens[:, 1:]\n",
    "\n",
    "        target_mask, token_padding_mask = model.create_masks(tokens_input)\n",
    "        target_mask = target_mask.to(DEVICE)\n",
    "        token_padding_mask = token_padding_mask.to(DEVICE)\n",
    "        logits = model(spectrogram, tuning, tokens_input, target_mask, token_padding_mask)\n",
    "        logits = logits.permute(1, 2, 0)\n",
    "\n",
    "        loss = loss_fn(logits, tokens_expected)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item()\n",
    "        f1_accuracy += metric(logits,tokens_expected)\n",
    "        count+=1\n",
    "        metrics_to_save = {\n",
    "            \"val_loss\": losses/count,\n",
    "            \"f1_acc_val\":f1_accuracy/count\n",
    "        }\n",
    "        if count % config[\"LogInterval\"] == 0:\n",
    "            wandb.log(metrics_to_save)\n",
    "        pbar.set_postfix(metrics_to_save)\n",
    "\n",
    "    losses = losses/len(val_dataloader)\n",
    "    checkpoint_saver(model, epoch, losses,optimizer,True)\n",
    "    return losses\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# load_model(\"./model_weights/GuitarModel_epoch9.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mdraguve\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.9"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\ritwi\\Github\\MusicTranscription\\wandb\\run-20230202_085246-4bazd1ic</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/draguve/MusicTranscription/runs/4bazd1ic\" target=\"_blank\">glistening-rat-4</a></strong> to <a href=\"https://wandb.ai/draguve/MusicTranscription\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href=\"https://wandb.ai/draguve/MusicTranscription\" target=\"_blank\">https://wandb.ai/draguve/MusicTranscription</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href=\"https://wandb.ai/draguve/MusicTranscription/runs/4bazd1ic\" target=\"_blank\">https://wandb.ai/draguve/MusicTranscription/runs/4bazd1ic</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 1:   0%|          | 0/30772 [00:09<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f79374c537fb462cbaeac026232b9511"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 1:   0%|          | 0/3420 [00:09<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f26a2c99b4a43b0b082b13ab7cb6600"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 2.133, Val loss: 1.580, Epoch time = 3860.727s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 2:   0%|          | 0/30772 [00:08<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a611580461dd4f57beac22e8cd936423"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 2:   0%|          | 0/3420 [00:08<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac710922b9164908b3b4de3135e81863"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 1.601, Val loss: 1.498, Epoch time = 3806.906s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 3:   0%|          | 0/30772 [00:07<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b047a45d90c940dea50beb531aa01fbf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 3:   0%|          | 0/3420 [00:08<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2db235182934a52a77c6f5ca468bdea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 1.543, Val loss: 1.444, Epoch time = 3815.146s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 4:   0%|          | 0/30772 [00:08<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c91d6f47505049ccb40a8e511e366523"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 4:   0%|          | 0/3420 [00:08<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63a83d823a0d47ab9ebea08254f51f42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train loss: 1.503, Val loss: 1.407, Epoch time = 3835.696s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 5:   0%|          | 0/30772 [00:08<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aada1f78cf2f4831853b3f926271f9c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 5:   0%|          | 0/3420 [00:08<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "427eef92fc1e44159a7fa63e06cb704f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 1.475, Val loss: 1.376, Epoch time = 3834.616s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 6:   0%|          | 0/30772 [00:08<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9aaf50ae8f434cc782a30fbb0dfe567a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 6:   0%|          | 0/3420 [00:08<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9dd0d25339124a5c8dae563ad66ad431"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train loss: 1.453, Val loss: 1.353, Epoch time = 3855.631s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 7:   0%|          | 0/30772 [01:08<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "575599e83b314845b169cb7c908f511a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 7:   0%|          | 0/3420 [00:08<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b893304d284d48539f4f69727d406759"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train loss: 1.435, Val loss: 1.330, Epoch time = 3885.404s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 8:   0%|          | 0/30772 [00:08<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ed77180c2ea4b79a72ec79c7f036f91"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 8:   0%|          | 0/3420 [00:08<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07c0a2eff7f94496b99458b7dbc0e202"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train loss: 1.419, Val loss: 1.310, Epoch time = 3870.662s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 9:   0%|          | 0/30772 [00:08<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59e309a2afbd4c178510375bf5fe820b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Eval 9:   0%|          | 0/3420 [01:09<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7435ef75be964ba8ba055b49c7f6f374"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train loss: 1.405, Val loss: 1.292, Epoch time = 3897.942s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 10:   0%|          | 0/30772 [00:08<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "745bf939588a4795929becf3e92096ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x00000219E70CFD90>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1430, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\multiprocessing\\process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\multiprocessing\\popen_spawn_win32.py\", line 108, in wait\n",
      "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_acc_train</td><td>▁▂▄▅████████████████████████████████████</td></tr><tr><td>f1_acc_val</td><td>▂▁▁▃▃▃▄▅▄▆▆▅▆▆▆▆▇▇█████████</td></tr><tr><td>train_loss</td><td>█▆▅▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>███▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_acc_train</td><td>0.01576</td></tr><tr><td>f1_acc_val</td><td>0.01694</td></tr><tr><td>train_loss</td><td>1.39245</td></tr><tr><td>val_loss</td><td>1.29568</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">glistening-rat-4</strong> at: <a href=\"https://wandb.ai/draguve/MusicTranscription/runs/4bazd1ic\" target=\"_blank\">https://wandb.ai/draguve/MusicTranscription/runs/4bazd1ic</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230202_085246-4bazd1ic\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_EPOCHS = 15\n",
    "checkpoint_saver = CheckpointSaver(dirpath='./model_weights', decreasing=True, top_n=5)\n",
    "config = {\n",
    "    \"EMB_SIZE\":EMB_SIZE,\n",
    "    \"NHEAD\":NHEAD,\n",
    "    \"FFN_HID_DIM\":FFN_HID_DIM,\n",
    "    \"BATCH_SIZE\":BATCH_SIZE,\n",
    "    \"NUM_ENCODER_LAYERS\":NUM_ENCODER_LAYERS,\n",
    "    \"NUM_DECODER_LAYERS\":NUM_DECODER_LAYERS,\n",
    "    \"VocabSize\":dataset.vocabSize,\n",
    "    \"TotalSize\":dataset.size,\n",
    "    \"MaxTokens\":dataset.maxTokens,\n",
    "    \"NumberOfTimeTokensPerSecond\":dataset.numberOfTimeTokensPerSecond,\n",
    "    \"SpectrogramSizeInSeconds\":dataset.spectrogramSizeInSeconds,\n",
    "    \"SampleRate\":dataset.sample_rate,\n",
    "    \"LearningRate\":LEARNING_RATE,\n",
    "    \"DataSetFile\":DatasetFile,\n",
    "    \"LogInterval\":1000\n",
    "}\n",
    "\n",
    "with wandb.init(project=\"MusicTranscription\",config=config,notes=f\"CheckpointID : {checkpoint_saver.run_id}\"):\n",
    "    wandb.watch(model, log_freq=config[\"LogInterval\"],log_graph=True,criterion=loss_fn)\n",
    "    for epoch in range(epoch, NUM_EPOCHS+1):\n",
    "        start_time = timer()\n",
    "        train_loss = train_epoch(model, optimizer,epoch)\n",
    "        end_time = timer()\n",
    "        val_loss = evaluate(model,epoch,checkpoint_saver,optimizer)\n",
    "        print(f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(Tokenizer,model,spectrogram : torch.tensor,tuning : torch.tensor,arrangement : str,capo : float):\n",
    "    model.eval()\n",
    "    SOS_token = Tokenizer.sosToken\n",
    "    EOS_token = Tokenizer.eosToken\n",
    "    max_length = dataset.maxTokens\n",
    "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=DEVICE)\n",
    "    tuningAndArrangement = ArrangementUtils.getArrangementTensor(tuning,arrangement,capo)\n",
    "    tuningAndArrangement = torch.unsqueeze(tuningAndArrangement,dim=0).to(DEVICE)\n",
    "    spectrogram = torch.unsqueeze(spectrogram,dim=0)\n",
    "    spectrogram = spectrogram.to(DEVICE)\n",
    "    for _ in range(max_length):\n",
    "        # Get source mask\n",
    "        target_mask, token_padding_mask = model.create_masks(y_input)\n",
    "        target_mask = target_mask.to(DEVICE)\n",
    "        token_padding_mask = token_padding_mask.to(DEVICE)\n",
    "\n",
    "        # token_padding_mask = torch.unsqueeze(token_padding_mask,dim=0)\n",
    "        pred = model(spectrogram, tuningAndArrangement, y_input, target_mask, token_padding_mask)\n",
    "\n",
    "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
    "        next_item = torch.tensor([[next_item]], device=DEVICE)\n",
    "\n",
    "        # Concatenate previous input with predicted best word\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.view(-1).tolist()\n",
    "\n",
    "def get_spectrogram(filepath,location_in_secs):\n",
    "    info = torchaudio.info(filepath)\n",
    "    file_sample_rate = info.sample_rate\n",
    "    file_start_offset = int(location_in_secs * file_sample_rate)\n",
    "    file_number_samples_to_read = int(dataset.spectrogramSizeInSeconds * file_sample_rate)\n",
    "    waveform, sample_rate = torchaudio.load(filepath, normalize=True, frame_offset=file_start_offset,\n",
    "                                            num_frames=file_number_samples_to_read)\n",
    "    if waveform.size(1) != file_number_samples_to_read:\n",
    "        # print(song, index, sectionIndex)\n",
    "        return None\n",
    "        # raise Exception(\"Read Less than expected\")\n",
    "\n",
    "\n",
    "    if sample_rate != dataset.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, orig_freq=file_sample_rate, new_freq=dataset.sample_rate)\n",
    "    waveform = mel_spectrogram(waveform)\n",
    "    return waveform\n",
    "\n",
    "def predict_from_file(filename,location_in_time):\n",
    "    Tokenizer = GuitarTokenizer(dataset.spectrogramSizeInSeconds,dataset.numberOfTimeTokensPerSecond)\n",
    "    spectrogram = get_spectrogram(filename,location_in_time)\n",
    "    tokens = predict(Tokenizer,model,spectrogram,ArrangementUtils.DSharp_Standard,\"lead\",0)\n",
    "    for i in tokens:\n",
    "        pprint(Tokenizer.encoder.decode(i))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sos', [True])\n",
      "('eot', [True])\n",
      "('silence', [True])\n",
      "('eos', [True])\n"
     ]
    }
   ],
   "source": [
    "predict_from_file(r\"C:\\Users\\ritwi\\Github\\MusicTranscription\\Downloads\\greewelc_p\\greewelc.ogg\",25.0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
