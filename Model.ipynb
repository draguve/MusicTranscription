{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda : True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "from SongDataset import SongDataset\n",
    "from TranscriptionModel import GuitarModel\n",
    "from torch import nn\n",
    "import os\n",
    "import logging\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from TUtils import random_string\n",
    "from Tokenizer import GuitarTokenizer\n",
    "from SongDataset import ArrangementUtils\n",
    "import wandb\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "# check if tensorflow is working correctly\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "print(f\"Cuda : {torch.cuda.is_available()}\")\n",
    "\n",
    "os.environ['WANDB_MODE'] = 'offline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#TODO: load this from dataset file\n",
    "SAMPLE_RATE = 44100\n",
    "DatasetFile = \"./Trainsets/massive.hdf5\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from SongDataset import GuitarCollater\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    ")\n",
    "dataset = SongDataset(DatasetFile, sampleRate=SAMPLE_RATE)\n",
    "collate_fn = GuitarCollater(dataset.pad_token)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 64\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "LEARNING_RATE = 0.0001\n",
    "epoch = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "model = GuitarModel((BATCH_SIZE, 2, 128, 87),\n",
    "                    mel_spectrogram,\n",
    "                    emb_size=EMB_SIZE,\n",
    "                    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "                    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "                    multi_head_attention_size=NHEAD,\n",
    "                    dim_feedforward=FFN_HID_DIM,\n",
    "                    tgt_vocab_size=dataset.vocabSize)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# loss_fn = torch.nn.CrossEntropyLoss(ignore_index=dataset.pad_token)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n",
    "metric = MulticlassF1Score(num_classes=int(dataset.vocabSize)).to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class CheckpointSaver:\n",
    "    def __init__(self, dirpath, decreasing=True, top_n=5):\n",
    "        \"\"\"\n",
    "        dirpath: Directory path where to store all model weights\n",
    "        decreasing: If decreasing is `True`, then lower metric is better\n",
    "        top_n: Total number of models to track based on validation metric value\n",
    "        \"\"\"\n",
    "        if not os.path.exists(dirpath): os.makedirs(dirpath)\n",
    "        self.dirpath = dirpath\n",
    "        self.top_n = top_n\n",
    "        self.decreasing = decreasing\n",
    "        self.top_model_paths = []\n",
    "        self.best_metric_val = np.Inf if decreasing else -np.Inf\n",
    "        self.run_id = random_string()\n",
    "\n",
    "    def __call__(self, model, epoch, metric_val,optimizer,force_save=False):\n",
    "        model_path = os.path.join(self.dirpath, model.__class__.__name__ + f'_epoch{epoch}_run{self.run_id}.pt')\n",
    "        save = metric_val<self.best_metric_val if self.decreasing else metric_val>self.best_metric_val\n",
    "        if save or force_save:\n",
    "            logging.info(f\"Current metric value better than {metric_val} better than best {self.best_metric_val}, saving model at {model_path}\")\n",
    "            self.best_metric_val = metric_val\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            # 'loss': loss,\n",
    "            }, model_path)\n",
    "            self.top_model_paths.append({'path': model_path, 'score': metric_val})\n",
    "            self.top_model_paths = sorted(self.top_model_paths, key=lambda o: o['score'], reverse=not self.decreasing)\n",
    "        if len(self.top_model_paths)>self.top_n:\n",
    "            self.cleanup()\n",
    "\n",
    "    def cleanup(self):\n",
    "        to_remove = self.top_model_paths[self.top_n:]\n",
    "        logging.info(f\"Removing extra models.. {to_remove}\")\n",
    "        for o in to_remove:\n",
    "            os.remove(o['path'])\n",
    "        self.top_model_paths = self.top_model_paths[:self.top_n]\n",
    "\n",
    "def save_model(PATH):\n",
    "    torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    # 'loss': loss,\n",
    "    }, PATH)\n",
    "\n",
    "def load_model(PATH):\n",
    "    global model\n",
    "    global optimizer\n",
    "    global epoch\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']+1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(dataset, [0.9,0.1], generator=torch.Generator().manual_seed(47))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, epoch):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    count = 0\n",
    "    f1_accuracy = 0\n",
    "    train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE,shuffle=True,num_workers=4,collate_fn=collate_fn)\n",
    "\n",
    "    for spectrogram, tuning, tokens,oneHot in (pbar := tqdm(train_dataloader,desc=f\"Epoch {epoch}\")):\n",
    "        spectrogram = spectrogram.to(DEVICE)\n",
    "        tuning = tuning.to(DEVICE)\n",
    "        tokens = tokens.to(DEVICE)\n",
    "        oneHot = oneHot.to(DEVICE)\n",
    "\n",
    "        tokens_input = tokens[:, :-1]\n",
    "        tokens_expected = tokens[:, 1:]\n",
    "\n",
    "        target_mask, token_padding_mask = model.create_masks(tokens_input)\n",
    "\n",
    "        target_mask = target_mask.to(DEVICE)\n",
    "        token_padding_mask = token_padding_mask.to(DEVICE)\n",
    "\n",
    "        logits = model(spectrogram, tuning, tokens_input, target_mask, token_padding_mask)\n",
    "        # logits = logits.permute(1, 2, 0)\n",
    "        # output = torch.argmax(p,dim=2,keepdim=True).squeeze(-1)\n",
    "        # output = output.permute(1, 0)\n",
    "        # expected_output = torch.nn.functional.one_hot(tokens_expected, num_classes=dataset.vocabSize)\n",
    "        # expected_output = expected_output.permute(1,0,2).type(torch.float)\n",
    "        # -------------------------------------------\n",
    "\n",
    "        output = torch.nn.functional.softmax(logits, dim=2)\n",
    "        loss = loss_fn(output, oneHot)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item()\n",
    "        # f1_accuracy += metric(output,oneHot)\n",
    "        count+=1\n",
    "        metrics_to_save = {\n",
    "            \"train_loss\": losses/count,\n",
    "        #     \"f1_acc_train\":f1_accuracy/count\n",
    "        }\n",
    "        if count % config[\"LogInterval\"] == 0:\n",
    "            wandb.log(metrics_to_save)\n",
    "        pbar.set_postfix(metrics_to_save)\n",
    "        # break\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model,epoch,checkpoint_saver,optimizer):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    count=0\n",
    "    f1_accuracy = 0\n",
    "\n",
    "    val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE,shuffle=True,num_workers=4,collate_fn=collate_fn)\n",
    "    for spectrogram, tuning, tokens,oneHot in (pbar := tqdm(val_dataloader,desc=f\"Eval {epoch}\")):\n",
    "        spectrogram = spectrogram.to(DEVICE)\n",
    "        tuning = tuning.to(DEVICE)\n",
    "        tokens = tokens.to(DEVICE)\n",
    "        oneHot = oneHot.to(DEVICE)\n",
    "\n",
    "        tokens_input = tokens[:, :-1]\n",
    "        tokens_expected = tokens[:, 1:]\n",
    "\n",
    "        target_mask, token_padding_mask = model.create_masks(tokens_input)\n",
    "        target_mask = target_mask.to(DEVICE)\n",
    "        token_padding_mask = token_padding_mask.to(DEVICE)\n",
    "        logits = model(spectrogram, tuning, tokens_input, target_mask, token_padding_mask)\n",
    "        # p = torch.nn.functional.softmax(logits, dim=2)\n",
    "        # output = torch.argmax(p,dim=2,keepdim=True).squeeze(-1)\n",
    "        # output = output.permute(1, 0)\n",
    "        output = torch.nn.functional.softmax(logits, dim=2)\n",
    "        loss = loss_fn(output, oneHot)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item()\n",
    "        f1_accuracy += metric(output,oneHot)\n",
    "        count+=1\n",
    "        metrics_to_save = {\n",
    "            \"val_loss\": losses/count,\n",
    "            \"f1_acc_val\":f1_accuracy/count\n",
    "        }\n",
    "        if count % config[\"LogInterval\"] == 0:\n",
    "            wandb.log(metrics_to_save)\n",
    "        pbar.set_postfix(metrics_to_save)\n",
    "        break\n",
    "\n",
    "    losses = losses/len(val_dataloader)\n",
    "    checkpoint_saver(model, epoch, losses,optimizer,True)\n",
    "    return losses\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# load_model(\"./model_weights/massive_first_test_0epoch.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.9"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 1:   0%|          | 0/254902 [00:11<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00feab23332f4aaea84e6e7787071e8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "checkpoint_saver = CheckpointSaver(dirpath='./model_weights', decreasing=True, top_n=5)\n",
    "config = {\n",
    "    \"EMB_SIZE\":EMB_SIZE,\n",
    "    \"NHEAD\":NHEAD,\n",
    "    \"FFN_HID_DIM\":FFN_HID_DIM,\n",
    "    \"BATCH_SIZE\":BATCH_SIZE,\n",
    "    \"NUM_ENCODER_LAYERS\":NUM_ENCODER_LAYERS,\n",
    "    \"NUM_DECODER_LAYERS\":NUM_DECODER_LAYERS,\n",
    "    \"VocabSize\":dataset.vocabSize,\n",
    "    \"TotalSize\":dataset.size,\n",
    "    \"MaxTokens\":dataset.maxTokens,\n",
    "    \"NumberOfTimeTokensPerSecond\":dataset.numberOfTimeTokensPerSecond,\n",
    "    \"SpectrogramSizeInSeconds\":dataset.spectrogramSizeInSeconds,\n",
    "    \"SampleRate\":dataset.sample_rate,\n",
    "    \"LearningRate\":LEARNING_RATE,\n",
    "    \"DataSetFile\":DatasetFile,\n",
    "    \"LogInterval\":1000\n",
    "}\n",
    "\n",
    "with profile(activities=[\n",
    "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        with wandb.init(project=\"MusicTranscription\",config=config,notes=f\"CheckpointID : {checkpoint_saver.run_id}\"):\n",
    "            wandb.watch(model, log_freq=config[\"LogInterval\"],log_graph=True,criterion=loss_fn)\n",
    "            for epoch in range(epoch, NUM_EPOCHS+1):\n",
    "                start_time = timer()\n",
    "                with record_function(\"model_inference\"):\n",
    "                    train_loss = train_epoch(model, optimizer,epoch)\n",
    "                end_time = timer()\n",
    "                # val_loss = evaluate(model,epoch,checkpoint_saver,optimizer)\n",
    "                # print(f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\")\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def predict(Tokenizer,model,spectrogram : torch.tensor,tuning : torch.tensor,arrangement : str,capo : float):\n",
    "    model.eval()\n",
    "    SOS_token = Tokenizer.sosToken\n",
    "    EOS_token = Tokenizer.eosToken\n",
    "    max_length = dataset.maxTokens\n",
    "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=DEVICE)\n",
    "    tuningAndArrangement = ArrangementUtils.getArrangementTensor(tuning,arrangement,capo)\n",
    "    tuningAndArrangement = torch.unsqueeze(tuningAndArrangement,dim=0).to(DEVICE)\n",
    "    spectrogram = torch.unsqueeze(spectrogram,dim=0)\n",
    "    spectrogram = spectrogram.to(DEVICE)\n",
    "    for _ in range(max_length):\n",
    "        # Get source mask\n",
    "        target_mask, token_padding_mask = model.create_masks(y_input)\n",
    "        target_mask = target_mask.to(DEVICE)\n",
    "        token_padding_mask = token_padding_mask.to(DEVICE)\n",
    "\n",
    "        # token_padding_mask = torch.unsqueeze(token_padding_mask,dim=0)\n",
    "        pred = model(spectrogram, tuningAndArrangement, y_input, target_mask, token_padding_mask)\n",
    "\n",
    "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
    "        next_item = torch.tensor([[next_item]], device=DEVICE)\n",
    "\n",
    "        # Concatenate previous input with predicted best word\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.view(-1).tolist()\n",
    "\n",
    "def get_spectrogram(filepath,location_in_secs):\n",
    "    info = torchaudio.info(filepath)\n",
    "    file_sample_rate = info.sample_rate\n",
    "    file_start_offset = int(location_in_secs * file_sample_rate)\n",
    "    file_number_samples_to_read = int(dataset.spectrogramSizeInSeconds * file_sample_rate)\n",
    "    waveform, sample_rate = torchaudio.load(filepath, normalize=True, frame_offset=file_start_offset,\n",
    "                                            num_frames=file_number_samples_to_read)\n",
    "    if waveform.size(1) != file_number_samples_to_read:\n",
    "        # print(song, index, sectionIndex)\n",
    "        return None\n",
    "        # raise Exception(\"Read Less than expected\")\n",
    "\n",
    "\n",
    "    if sample_rate != dataset.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, orig_freq=file_sample_rate, new_freq=dataset.sample_rate)\n",
    "    # waveform = mel_spectrogram(waveform)\n",
    "    return waveform\n",
    "\n",
    "def get_waveform_all(filepath):\n",
    "    waveform, sample_rate = torchaudio.load(filepath, normalize=True)\n",
    "    if sample_rate != dataset.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, orig_freq=sample_rate, new_freq=dataset.sample_rate)\n",
    "    # waveform = mel_spectrogram(waveform)\n",
    "    file_number_samples_to_read = int(dataset.spectrogramSizeInSeconds * dataset.sample_rate)\n",
    "    return torch.split(waveform,file_number_samples_to_read,dim=1)\n",
    "\n",
    "def predict_from_file(filename,location_in_time):\n",
    "    Tokenizer = GuitarTokenizer(dataset.spectrogramSizeInSeconds,dataset.numberOfTimeTokensPerSecond)\n",
    "    spectrogram = get_spectrogram(filename,location_in_time)\n",
    "    tokens = predict(Tokenizer,model,spectrogram,ArrangementUtils.DSharp_Standard,\"lead\",0)\n",
    "    for i in tokens:\n",
    "        pprint(Tokenizer.encoder.decode(i))\n",
    "\n",
    "def predict_entire_file(filename):\n",
    "    Tokenizer = GuitarTokenizer(dataset.spectrogramSizeInSeconds,dataset.numberOfTimeTokensPerSecond)\n",
    "    for x in get_waveform_all(filename):\n",
    "        tokens = predict(Tokenizer,model,x,ArrangementUtils.DSharp_Standard,\"lead\",0)\n",
    "        for token in tokens:\n",
    "            print(Tokenizer.encoder.decode(token))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict_entire_file(r\"C:\\Users\\ritwi\\Github\\MusicTranscription\\Downloads2\\S_Tier\\greewelc_p\\greewelc.ogg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict_from_file(r\"C:\\Users\\ritwi\\Github\\MusicTranscription\\Downloads2\\S_Tier\\greewelc_p\\greewelc.ogg\",25.0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_model(\"model_weights/massive_first_test_0epoch.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
