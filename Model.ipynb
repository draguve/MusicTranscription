{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda : True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "from SongDataset import SongDataset\n",
    "from TranscriptionModel import GuitarModel\n",
    "from torch import nn\n",
    "import os\n",
    "import logging\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from TUtils import random_string\n",
    "from Tokenizer import GuitarTokenizer\n",
    "from SongDataset import ArrangementUtils\n",
    "import wandb\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "\n",
    "\n",
    "# check if tensorflow is working correctly\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "print(f\"Cuda : {torch.cuda.is_available()}\")\n",
    "\n",
    "os.environ['WANDB_MODE'] = 'offline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#TODO: load this from dataset file\n",
    "SAMPLE_RATE = 44100\n",
    "DatasetFile = \"./Trainsets/massive.hdf5\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from SongDataset import GuitarCollater\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    ")\n",
    "dataset = SongDataset(DatasetFile, sampleRate=SAMPLE_RATE)\n",
    "collate_fn = GuitarCollater(dataset.pad_token)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 16\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "LEARNING_RATE = 0.0001\n",
    "epoch = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "\n",
    "model = GuitarModel((BATCH_SIZE, 2, 128, 87),\n",
    "                    mel_spectrogram,\n",
    "                    emb_size=EMB_SIZE,\n",
    "                    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "                    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "                    multi_head_attention_size=NHEAD,\n",
    "                    dim_feedforward=FFN_HID_DIM,\n",
    "                    tgt_vocab_size=dataset.vocabSize)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# loss_fn = torch.nn.CrossEntropyLoss(ignore_index=dataset.pad_token)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n",
    "metric = MulticlassF1Score(num_classes=int(dataset.vocabSize)).to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class CheckpointSaver:\n",
    "    def __init__(self, dirpath, decreasing=True, top_n=5):\n",
    "        \"\"\"\n",
    "        dirpath: Directory path where to store all model weights\n",
    "        decreasing: If decreasing is `True`, then lower metric is better\n",
    "        top_n: Total number of models to track based on validation metric value\n",
    "        \"\"\"\n",
    "        if not os.path.exists(dirpath): os.makedirs(dirpath)\n",
    "        self.dirpath = dirpath\n",
    "        self.top_n = top_n\n",
    "        self.decreasing = decreasing\n",
    "        self.top_model_paths = []\n",
    "        self.best_metric_val = np.Inf if decreasing else -np.Inf\n",
    "        self.run_id = random_string()\n",
    "\n",
    "    def __call__(self, model, epoch, metric_val,optimizer,force_save=False):\n",
    "        model_path = os.path.join(self.dirpath, model.__class__.__name__ + f'_epoch{epoch}_run{self.run_id}.pt')\n",
    "        save = metric_val<self.best_metric_val if self.decreasing else metric_val>self.best_metric_val\n",
    "        if save or force_save:\n",
    "            logging.info(f\"Current metric value better than {metric_val} better than best {self.best_metric_val}, saving model at {model_path}\")\n",
    "            self.best_metric_val = metric_val\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            # 'loss': loss,\n",
    "            }, model_path)\n",
    "            self.top_model_paths.append({'path': model_path, 'score': metric_val})\n",
    "            self.top_model_paths = sorted(self.top_model_paths, key=lambda o: o['score'], reverse=not self.decreasing)\n",
    "        if len(self.top_model_paths)>self.top_n:\n",
    "            self.cleanup()\n",
    "\n",
    "    def cleanup(self):\n",
    "        to_remove = self.top_model_paths[self.top_n:]\n",
    "        logging.info(f\"Removing extra models.. {to_remove}\")\n",
    "        for o in to_remove:\n",
    "            os.remove(o['path'])\n",
    "        self.top_model_paths = self.top_model_paths[:self.top_n]\n",
    "\n",
    "def save_model(PATH):\n",
    "    torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    # 'loss': loss,\n",
    "    }, PATH)\n",
    "\n",
    "def load_model(PATH):\n",
    "    global model\n",
    "    global optimizer\n",
    "    global epoch\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']+1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(dataset, [0.9,0.1], generator=torch.Generator().manual_seed(47))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, epoch):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    count = 0\n",
    "    f1_accuracy = 0\n",
    "    train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE,shuffle=True,num_workers=2,collate_fn=collate_fn)\n",
    "\n",
    "    for spectrogram, tuning, tokens in (pbar := tqdm(train_dataloader,desc=f\"Epoch {epoch}\")):\n",
    "        spectrogram = spectrogram.to(DEVICE)\n",
    "        tuning = tuning.to(DEVICE)\n",
    "        tokens = tokens.to(DEVICE)\n",
    "\n",
    "        tokens_input = tokens[:, :-1]\n",
    "        tokens_expected = tokens[:, 1:]\n",
    "\n",
    "        target_mask, token_padding_mask = model.create_masks(tokens_input)\n",
    "\n",
    "        target_mask = target_mask.to(DEVICE)\n",
    "        token_padding_mask = token_padding_mask.to(DEVICE)\n",
    "\n",
    "        logits = model(spectrogram, tuning, tokens_input, target_mask, token_padding_mask)\n",
    "        # logits = logits.permute(1, 2, 0)\n",
    "        output = torch.nn.functional.softmax(logits, dim=2)\n",
    "        expected_output = torch.nn.functional.one_hot(tokens_expected, num_classes=dataset.vocabSize)\n",
    "        # output = torch.argmax(p,dim=2,keepdim=True).squeeze(-1)\n",
    "        # output = output.permute(1, 0)\n",
    "        expected_output = expected_output.permute(1,0,2).type(torch.float)\n",
    "        loss = loss_fn(output, expected_output)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item()\n",
    "        f1_accuracy += metric(output,expected_output)\n",
    "        count+=1\n",
    "        metrics_to_save = {\n",
    "            \"train_loss\": losses/count,\n",
    "            \"f1_acc_train\":f1_accuracy/count\n",
    "        }\n",
    "        if count % config[\"LogInterval\"] == 0:\n",
    "            wandb.log(metrics_to_save)\n",
    "        pbar.set_postfix(metrics_to_save)\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model,epoch,checkpoint_saver,optimizer):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    count=0\n",
    "    f1_accuracy = 0\n",
    "\n",
    "    val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE,shuffle=True,num_workers=2,collate_fn=collate_fn)\n",
    "    for spectrogram, tuning, tokens in (pbar := tqdm(val_dataloader,desc=f\"Eval {epoch}\")):\n",
    "        spectrogram = spectrogram.to(DEVICE)\n",
    "        tuning = tuning.to(DEVICE)\n",
    "        tokens = tokens.to(DEVICE)\n",
    "\n",
    "        tokens_input = tokens[:, :-1]\n",
    "        tokens_expected = tokens[:, 1:]\n",
    "\n",
    "        target_mask, token_padding_mask = model.create_masks(tokens_input)\n",
    "        target_mask = target_mask.to(DEVICE)\n",
    "        token_padding_mask = token_padding_mask.to(DEVICE)\n",
    "        logits = model(spectrogram, tuning, tokens_input, target_mask, token_padding_mask)\n",
    "        # p = torch.nn.functional.softmax(logits, dim=2)\n",
    "        # output = torch.argmax(p,dim=2,keepdim=True).squeeze(-1)\n",
    "        # output = output.permute(1, 0)\n",
    "        output = torch.nn.functional.softmax(logits, dim=2)\n",
    "        expected_output = torch.nn.functional.one_hot(tokens_expected, num_classes=dataset.vocabSize)\n",
    "        expected_output = expected_output.permute(1,0,2)\n",
    "        expected_output = expected_output.type(torch.float)\n",
    "        loss = loss_fn(output, expected_output)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item()\n",
    "        f1_accuracy += metric(output,expected_output)\n",
    "        count+=1\n",
    "        metrics_to_save = {\n",
    "            \"val_loss\": losses/count,\n",
    "            \"f1_acc_val\":f1_accuracy/count\n",
    "        }\n",
    "        if count % config[\"LogInterval\"] == 0:\n",
    "            wandb.log(metrics_to_save)\n",
    "        pbar.set_postfix(metrics_to_save)\n",
    "\n",
    "    losses = losses/len(val_dataloader)\n",
    "    checkpoint_saver(model, epoch, losses,optimizer,True)\n",
    "    return losses\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# load_model(\"./model_weights/massive_first_test_0epoch.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.9"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 1:   0%|          | 0/1019605 [00:07<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "678dacea9d47476aa6c20ba71eb47bde"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "You can sync this run to the cloud by running:<br/><code>wandb sync C:\\Users\\ritwi\\Github\\MusicTranscription\\wandb\\offline-run-20230220_203419-5xni8mfj<code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\offline-run-20230220_203419-5xni8mfj\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 25\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epoch, NUM_EPOCHS\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m     24\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[1;32m---> 25\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[0;32m     27\u001B[0m     val_loss \u001B[38;5;241m=\u001B[39m evaluate(model,epoch,checkpoint_saver,optimizer)\n",
      "Cell \u001B[1;32mIn[18], line 41\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, optimizer, epoch)\u001B[0m\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m count \u001B[38;5;241m%\u001B[39m config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLogInterval\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     40\u001B[0m         wandb\u001B[38;5;241m.\u001B[39mlog(metrics_to_save)\n\u001B[1;32m---> 41\u001B[0m     \u001B[43mpbar\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_postfix\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmetrics_to_save\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m losses \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_dataloader)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\tqdm\\std.py:1439\u001B[0m, in \u001B[0;36mtqdm.set_postfix\u001B[1;34m(self, ordered_dict, refresh, **kwargs)\u001B[0m\n\u001B[0;32m   1437\u001B[0m     \u001B[38;5;66;03m# Else for any other type, try to get the string conversion\u001B[39;00m\n\u001B[0;32m   1438\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(postfix[key], _basestring):\n\u001B[1;32m-> 1439\u001B[0m         postfix[key] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpostfix\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1440\u001B[0m     \u001B[38;5;66;03m# Else if it's a string, don't need to preprocess anything\u001B[39;00m\n\u001B[0;32m   1441\u001B[0m \u001B[38;5;66;03m# Stitch together to get the final postfix\u001B[39;00m\n\u001B[0;32m   1442\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostfix \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(key \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m postfix[key]\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[0;32m   1443\u001B[0m                          \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m postfix\u001B[38;5;241m.\u001B[39mkeys())\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\_tensor.py:427\u001B[0m, in \u001B[0;36mTensor.__repr__\u001B[1;34m(self, tensor_contents)\u001B[0m\n\u001B[0;32m    423\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    424\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__repr__\u001B[39m, (\u001B[38;5;28mself\u001B[39m,), \u001B[38;5;28mself\u001B[39m, tensor_contents\u001B[38;5;241m=\u001B[39mtensor_contents\n\u001B[0;32m    425\u001B[0m     )\n\u001B[0;32m    426\u001B[0m \u001B[38;5;66;03m# All strings are unicode in Python 3.\u001B[39;00m\n\u001B[1;32m--> 427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tensor_str\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_str\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor_contents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_contents\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\_tensor_str.py:637\u001B[0m, in \u001B[0;36m_str\u001B[1;34m(self, tensor_contents)\u001B[0m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m    636\u001B[0m     guard \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_DisableFuncTorch()\n\u001B[1;32m--> 637\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_str_intern\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor_contents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_contents\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\_tensor_str.py:568\u001B[0m, in \u001B[0;36m_str_intern\u001B[1;34m(inp, tensor_contents)\u001B[0m\n\u001B[0;32m    566\u001B[0m                     tensor_str \u001B[38;5;241m=\u001B[39m _tensor_str(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_dense(), indent)\n\u001B[0;32m    567\u001B[0m                 \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 568\u001B[0m                     tensor_str \u001B[38;5;241m=\u001B[39m \u001B[43m_tensor_str\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    570\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayout \u001B[38;5;241m!=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstrided:\n\u001B[0;32m    571\u001B[0m     suffixes\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayout=\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayout))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\_tensor_str.py:328\u001B[0m, in \u001B[0;36m_tensor_str\u001B[1;34m(self, indent)\u001B[0m\n\u001B[0;32m    324\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _tensor_str_with_formatter(\n\u001B[0;32m    325\u001B[0m         \u001B[38;5;28mself\u001B[39m, indent, summarize, real_formatter, imag_formatter\n\u001B[0;32m    326\u001B[0m     )\n\u001B[0;32m    327\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 328\u001B[0m     formatter \u001B[38;5;241m=\u001B[39m \u001B[43m_Formatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mget_summarized_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msummarize\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    329\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _tensor_str_with_formatter(\u001B[38;5;28mself\u001B[39m, indent, summarize, formatter)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\_tensor_str.py:115\u001B[0m, in \u001B[0;36m_Formatter.__init__\u001B[1;34m(self, tensor)\u001B[0m\n\u001B[0;32m    112\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_width \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_width, \u001B[38;5;28mlen\u001B[39m(value_str))\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 115\u001B[0m     nonzero_finite_vals \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmasked_select\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtensor_view\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misfinite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor_view\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m&\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtensor_view\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mne\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    117\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m nonzero_finite_vals\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    120\u001B[0m         \u001B[38;5;66;03m# no valid number, do nothing\u001B[39;00m\n\u001B[0;32m    121\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 3\n",
    "checkpoint_saver = CheckpointSaver(dirpath='./model_weights', decreasing=True, top_n=5)\n",
    "config = {\n",
    "    \"EMB_SIZE\":EMB_SIZE,\n",
    "    \"NHEAD\":NHEAD,\n",
    "    \"FFN_HID_DIM\":FFN_HID_DIM,\n",
    "    \"BATCH_SIZE\":BATCH_SIZE,\n",
    "    \"NUM_ENCODER_LAYERS\":NUM_ENCODER_LAYERS,\n",
    "    \"NUM_DECODER_LAYERS\":NUM_DECODER_LAYERS,\n",
    "    \"VocabSize\":dataset.vocabSize,\n",
    "    \"TotalSize\":dataset.size,\n",
    "    \"MaxTokens\":dataset.maxTokens,\n",
    "    \"NumberOfTimeTokensPerSecond\":dataset.numberOfTimeTokensPerSecond,\n",
    "    \"SpectrogramSizeInSeconds\":dataset.spectrogramSizeInSeconds,\n",
    "    \"SampleRate\":dataset.sample_rate,\n",
    "    \"LearningRate\":LEARNING_RATE,\n",
    "    \"DataSetFile\":DatasetFile,\n",
    "    \"LogInterval\":1000\n",
    "}\n",
    "\n",
    "with wandb.init(project=\"MusicTranscription\",config=config,notes=f\"CheckpointID : {checkpoint_saver.run_id}\"):\n",
    "    wandb.watch(model, log_freq=config[\"LogInterval\"],log_graph=True,criterion=loss_fn)\n",
    "    for epoch in range(epoch, NUM_EPOCHS+1):\n",
    "        start_time = timer()\n",
    "        train_loss = train_epoch(model, optimizer,epoch)\n",
    "        end_time = timer()\n",
    "        val_loss = evaluate(model,epoch,checkpoint_saver,optimizer)\n",
    "        print(f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(Tokenizer,model,spectrogram : torch.tensor,tuning : torch.tensor,arrangement : str,capo : float):\n",
    "    model.eval()\n",
    "    SOS_token = Tokenizer.sosToken\n",
    "    EOS_token = Tokenizer.eosToken\n",
    "    max_length = dataset.maxTokens\n",
    "    y_input = torch.tensor([[SOS_token]], dtype=torch.long, device=DEVICE)\n",
    "    tuningAndArrangement = ArrangementUtils.getArrangementTensor(tuning,arrangement,capo)\n",
    "    tuningAndArrangement = torch.unsqueeze(tuningAndArrangement,dim=0).to(DEVICE)\n",
    "    spectrogram = torch.unsqueeze(spectrogram,dim=0)\n",
    "    spectrogram = spectrogram.to(DEVICE)\n",
    "    for _ in range(max_length):\n",
    "        # Get source mask\n",
    "        target_mask, token_padding_mask = model.create_masks(y_input)\n",
    "        target_mask = target_mask.to(DEVICE)\n",
    "        token_padding_mask = token_padding_mask.to(DEVICE)\n",
    "\n",
    "        # token_padding_mask = torch.unsqueeze(token_padding_mask,dim=0)\n",
    "        pred = model(spectrogram, tuningAndArrangement, y_input, target_mask, token_padding_mask)\n",
    "\n",
    "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
    "        next_item = torch.tensor([[next_item]], device=DEVICE)\n",
    "\n",
    "        # Concatenate previous input with predicted best word\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.view(-1).tolist()\n",
    "\n",
    "def get_spectrogram(filepath,location_in_secs):\n",
    "    info = torchaudio.info(filepath)\n",
    "    file_sample_rate = info.sample_rate\n",
    "    file_start_offset = int(location_in_secs * file_sample_rate)\n",
    "    file_number_samples_to_read = int(dataset.spectrogramSizeInSeconds * file_sample_rate)\n",
    "    waveform, sample_rate = torchaudio.load(filepath, normalize=True, frame_offset=file_start_offset,\n",
    "                                            num_frames=file_number_samples_to_read)\n",
    "    if waveform.size(1) != file_number_samples_to_read:\n",
    "        # print(song, index, sectionIndex)\n",
    "        return None\n",
    "        # raise Exception(\"Read Less than expected\")\n",
    "\n",
    "\n",
    "    if sample_rate != dataset.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, orig_freq=file_sample_rate, new_freq=dataset.sample_rate)\n",
    "    # waveform = mel_spectrogram(waveform)\n",
    "    return waveform\n",
    "\n",
    "def get_waveform_all(filepath):\n",
    "    waveform, sample_rate = torchaudio.load(filepath, normalize=True)\n",
    "    if sample_rate != dataset.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, orig_freq=sample_rate, new_freq=dataset.sample_rate)\n",
    "    # waveform = mel_spectrogram(waveform)\n",
    "    file_number_samples_to_read = int(dataset.spectrogramSizeInSeconds * dataset.sample_rate)\n",
    "    return torch.split(waveform,file_number_samples_to_read,dim=1)\n",
    "\n",
    "def predict_from_file(filename,location_in_time):\n",
    "    Tokenizer = GuitarTokenizer(dataset.spectrogramSizeInSeconds,dataset.numberOfTimeTokensPerSecond)\n",
    "    spectrogram = get_spectrogram(filename,location_in_time)\n",
    "    tokens = predict(Tokenizer,model,spectrogram,ArrangementUtils.DSharp_Standard,\"lead\",0)\n",
    "    for i in tokens:\n",
    "        pprint(Tokenizer.encoder.decode(i))\n",
    "\n",
    "def predict_entire_file(filename):\n",
    "    Tokenizer = GuitarTokenizer(dataset.spectrogramSizeInSeconds,dataset.numberOfTimeTokensPerSecond)\n",
    "    for x in get_waveform_all(filename):\n",
    "        tokens = predict(Tokenizer,model,x,ArrangementUtils.DSharp_Standard,\"lead\",0)\n",
    "        for token in tokens:\n",
    "            print(Tokenizer.encoder.decode(token))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict_entire_file(r\"C:\\Users\\ritwi\\Github\\MusicTranscription\\Downloads2\\S_Tier\\greewelc_p\\greewelc.ogg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict_from_file(r\"C:\\Users\\ritwi\\Github\\MusicTranscription\\Downloads2\\S_Tier\\greewelc_p\\greewelc.ogg\",25.0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_model(\"model_weights/massive_first_test_0epoch.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
