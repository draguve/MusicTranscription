{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda : True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "from SongDataset import SongDataset\n",
    "from TranscriptionModel import GuitarModel\n",
    "from torch import nn\n",
    "import os\n",
    "import logging\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# check if tensorflow is working correctly\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "print(f\"Cuda : {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SAMPLE_RATE = 44100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from SongDataset import GuitarCollater\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    ")\n",
    "dataset = SongDataset(\"test.hdf5\", mel_spectrogram, sampleRate=SAMPLE_RATE)\n",
    "collate_fn = GuitarCollater(dataset.pad_token)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model = GuitarModel((BATCH_SIZE, 2, 128, 87),\n",
    "                        emb_size=EMB_SIZE,\n",
    "                        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "                        num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "                        multi_head_attention_size=NHEAD,\n",
    "                        dim_feedforward=FFN_HID_DIM,\n",
    "                        tgt_vocab_size=dataset.vocabSize)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "transformer = model.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=dataset.pad_token)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class CheckpointSaver:\n",
    "    def __init__(self, dirpath, decreasing=True, top_n=5):\n",
    "        \"\"\"\n",
    "        dirpath: Directory path where to store all model weights\n",
    "        decreasing: If decreasing is `True`, then lower metric is better\n",
    "        top_n: Total number of models to track based on validation metric value\n",
    "        \"\"\"\n",
    "        if not os.path.exists(dirpath): os.makedirs(dirpath)\n",
    "        self.dirpath = dirpath\n",
    "        self.top_n = top_n\n",
    "        self.decreasing = decreasing\n",
    "        self.top_model_paths = []\n",
    "        self.best_metric_val = np.Inf if decreasing else -np.Inf\n",
    "\n",
    "    def __call__(self, model, epoch, metric_val):\n",
    "        model_path = os.path.join(self.dirpath, model.__class__.__name__ + f'_epoch{epoch}.pt')\n",
    "        save = metric_val<self.best_metric_val if self.decreasing else metric_val>self.best_metric_val\n",
    "        if save:\n",
    "            logging.info(f\"Current metric value better than {metric_val} better than best {self.best_metric_val}, saving model at {model_path}\")\n",
    "            self.best_metric_val = metric_val\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            self.top_model_paths.append({'path': model_path, 'score': metric_val})\n",
    "            self.top_model_paths = sorted(self.top_model_paths, key=lambda o: o['score'], reverse=not self.decreasing)\n",
    "        if len(self.top_model_paths)>self.top_n:\n",
    "            self.cleanup()\n",
    "\n",
    "    def cleanup(self):\n",
    "        to_remove = self.top_model_paths[self.top_n:]\n",
    "        logging.info(f\"Removing extra models.. {to_remove}\")\n",
    "        for o in to_remove:\n",
    "            os.remove(o['path'])\n",
    "        self.top_model_paths = self.top_model_paths[:self.top_n]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(dataset, [0.9,0.1], generator=torch.Generator().manual_seed(42))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, epoch):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE,shuffle=True,num_workers=4,collate_fn=collate_fn)\n",
    "\n",
    "    for spectrogram, tuning, tokens in tqdm(train_dataloader,desc=f\"Epoch {epoch}\"):\n",
    "        spectrogram = spectrogram.to(DEVICE)\n",
    "        tuning = tuning.to(DEVICE)\n",
    "        tokens = tokens.to(DEVICE)\n",
    "\n",
    "        tokens_input = tokens[:, :-1]\n",
    "        tokens_expected = tokens[:, 1:]\n",
    "\n",
    "        target_mask, token_padding_mask = model.create_masks(tokens_input)\n",
    "        logits = model(spectrogram, tuning, tokens_input, target_mask, token_padding_mask)\n",
    "        logits = logits.permute(1, 2, 0)\n",
    "\n",
    "        loss = loss_fn(logits, tokens_expected)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model,epoch,checkpoint_saver):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE,shuffle=False,num_workers=4,collate_fn=collate_fn)\n",
    "    for spectrogram, tuning, tokens in tqdm(val_dataloader,desc=f\"Eval {epoch}\"):\n",
    "        spectrogram = spectrogram.to(DEVICE)\n",
    "        tuning = tuning.to(DEVICE)\n",
    "        tokens = tokens.to(DEVICE)\n",
    "\n",
    "        tokens_input = tokens[:, :-1]\n",
    "        tokens_expected = tokens[:, 1:]\n",
    "\n",
    "        target_mask, token_padding_mask = model.create_masks(tokens_input)\n",
    "        logits = model(spectrogram, tuning, tokens_input, target_mask, token_padding_mask)\n",
    "        logits = logits.permute(1, 2, 0)\n",
    "\n",
    "        loss = loss_fn(logits, tokens_expected)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.detach().item()\n",
    "\n",
    "    losses = losses/len(val_dataloader)\n",
    "    checkpoint_saver(model, epoch, losses)\n",
    "    return losses / len(val_dataloader)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch 1:   0%|          | 0/4711 [00:06<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ff8731118da48c0a80f02a501b3a309"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 295, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"C:\\Users\\ritwi\\Github\\MusicTranscription\\SongDataset\\__init__.py\", line 45, in __getitem__\n    waveform = torchaudio.functional.resample(waveform, orig_freq=file_sample_rate, new_freq=self.sample_rate)\n  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\site-packages\\torchaudio\\functional\\functional.py\", line 1600, in resample\n    resampled = _apply_sinc_resample_kernel(waveform, orig_freq, new_freq, gcd, kernel, width)\n  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\site-packages\\torchaudio\\functional\\functional.py\", line 1532, in _apply_sinc_resample_kernel\n    waveform = waveform.view(-1, shape[-1])\nRuntimeError: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, NUM_EPOCHS\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m      4\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[1;32m----> 5\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[0;32m      7\u001B[0m     val_loss \u001B[38;5;241m=\u001B[39m evaluate(transformer,epoch,checkpoint_saver)\n",
      "Cell \u001B[1;32mIn[9], line 6\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, optimizer, epoch)\u001B[0m\n\u001B[0;32m      3\u001B[0m losses \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m      4\u001B[0m train_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(train_set, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE,shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m,collate_fn\u001B[38;5;241m=\u001B[39mcollate_fn)\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m spectrogram, tuning, tokens \u001B[38;5;129;01min\u001B[39;00m tqdm(train_dataloader,desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m      7\u001B[0m     spectrogram \u001B[38;5;241m=\u001B[39m spectrogram\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[0;32m      8\u001B[0m     tuning \u001B[38;5;241m=\u001B[39m tuning\u001B[38;5;241m.\u001B[39mto(DEVICE)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\tqdm\\notebook.py:259\u001B[0m, in \u001B[0;36mtqdm_notebook.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    258\u001B[0m     it \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m(tqdm_notebook, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__iter__\u001B[39m()\n\u001B[1;32m--> 259\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m it:\n\u001B[0;32m    260\u001B[0m         \u001B[38;5;66;03m# return super(tqdm...) will not catch exception\u001B[39;00m\n\u001B[0;32m    261\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m    262\u001B[0m \u001B[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\tqdm\\std.py:1195\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1192\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1194\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1195\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1196\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1197\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1333\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1331\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1332\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[1;32m-> 1333\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1359\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m   1357\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[0;32m   1358\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[1;32m-> 1359\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1360\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\_utils.py:543\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    539\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    540\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[0;32m    541\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[0;32m    542\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m--> 543\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 295, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"C:\\Users\\ritwi\\Github\\MusicTranscription\\SongDataset\\__init__.py\", line 45, in __getitem__\n    waveform = torchaudio.functional.resample(waveform, orig_freq=file_sample_rate, new_freq=self.sample_rate)\n  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\site-packages\\torchaudio\\functional\\functional.py\", line 1600, in resample\n    resampled = _apply_sinc_resample_kernel(waveform, orig_freq, new_freq, gcd, kernel, width)\n  File \"C:\\Users\\ritwi\\anaconda3\\envs\\music\\lib\\site-packages\\torchaudio\\functional\\functional.py\", line 1532, in _apply_sinc_resample_kernel\n    waveform = waveform.view(-1, shape[-1])\nRuntimeError: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 18\n",
    "checkpoint_saver = CheckpointSaver(dirpath='./model_weights', decreasing=True, top_n=1)\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,epoch)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,epoch,checkpoint_saver)\n",
    "    print(f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
